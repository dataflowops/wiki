"use strict";(self.webpackChunkwiki=self.webpackChunkwiki||[]).push([[9504],{5638:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>a});const o=JSON.parse('{"id":"task-types/openai-completion","title":"OpenAI / Completion","description":"Workflow task type to call OpenAI\'s GPT model for text completion and summarization tasks.","source":"@site/docs/task-types/openai-completion.md","sourceDirName":"task-types","slug":"/task-types/openai-completion","permalink":"/docs/task-types/openai-completion","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"OpenAI / Completion","slug":"openai-completion"},"sidebar":"documentationSidebar","previous":{"title":"OpenAI / Audio / Transcription","permalink":"/docs/task-types/openai-audio-transcription"},"next":{"title":"Video / Extract Audio","permalink":"/docs/task-types/video-extract-audio"}}');var s=t(4848),i=t(8453);const r={title:"OpenAI / Completion",slug:"openai-completion"},l="OpenAI GPT",p={},a=[{value:"Inputs",id:"inputs",level:2},{value:"Outputs",id:"outputs",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"openai-gpt",children:"OpenAI GPT"})}),"\n",(0,s.jsx)(n.p,{children:"Workflow task type to call OpenAI's GPT model for text completion and summarization tasks."}),"\n",(0,s.jsx)(n.h2,{id:"inputs",children:"Inputs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"text"}),": The input text to process (",(0,s.jsx)(n.strong,{children:"required input"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"prompt"}),": Prompt to guide the model's output (",(0,s.jsx)(n.strong,{children:"required input"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"model"}),': The GPT model to use (e.g. "gpt-4", "gpt-3.5-turbo"). Defaults to "gpt-3.5-turbo".']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"max_tokens"}),": Maximum number of tokens to generate in the completion."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"temperature"}),": Controls randomness in the output (0-2). Higher values make output more random. Defaults to 0.7."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"top_p"}),": Controls diversity via nucleus sampling (0-1). Lower values make output more focused. Defaults to 1.0."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"frequency_penalty"}),": Reduces repetition of token sequences (-2 to 2). Positive values decrease repetition. Defaults to 0."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"presence_penalty"}),": Encourages discussing new topics (-2 to 2). Positive values increase topic diversity. Defaults to 0."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"outputs",children:"Outputs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"completion"}),": The generated text completion (",(0,s.jsx)(n.strong,{children:"default output"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"usage"}),": Token usage statistics for the request."]}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"example-usage",children:"Example Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "name": "summarize_text",\n  "type": "openai.completion",\n  "inputs": {\n    "text": "{{inputs.article_text}}",\n    "prompt": "Summarize this article in 3 bullet points",\n    "model": "gpt-4",\n    "max_tokens": 300,\n    "temperature": 0.5\n  }\n}\n'})}),"\n",(0,s.jsxs)(n.p,{children:["This example assumes that the workflow has an input named ",(0,s.jsx)(n.code,{children:"article_text"})," containing the text to summarize."]}),"\n",(0,s.jsx)(n.h1,{id:"specification",children:"Specification"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "specification": {\n    "type": "openai.completion",\n    "category": "ai",\n    "description": "Call OpenAI\'s GPT model to generate text completions",\n    "inputs": [\n      {\n        "name": "text",\n        "type": "text",\n        "required": true,\n        "description": "The input text to process",\n        "example": "Analyze the following article..."\n      },\n      {\n        "name": "prompt",\n        "type": "text",\n        "required": true,\n        "description": "Prompt to guide the model\'s output",\n        "example": "Summarize this text in 3 bullet points"\n      },\n      {\n        "name": "model",\n        "type": "text",\n        "required": false,\n        "default": "gpt-3.5-turbo",\n        "description": "The GPT model to use",\n        "example": "gpt-4"\n      },\n      {\n        "name": "max_tokens",\n        "type": "number",\n        "required": false,\n        "description": "Maximum number of tokens to generate",\n        "example": 300\n      },\n      {\n        "name": "temperature",\n        "type": "number",\n        "required": false,\n        "default": 0.7,\n        "description": "Controls randomness in the output (0-2)",\n        "example": 0.5\n      },\n      {\n        "name": "top_p",\n        "type": "number",\n        "required": false,\n        "default": 1.0,\n        "description": "Controls diversity via nucleus sampling (0-1)",\n        "example": 0.8\n      },\n      {\n        "name": "frequency_penalty",\n        "type": "number",\n        "required": false,\n        "default": 0.0,\n        "description": "Reduces repetition of token sequences (-2 to 2)",\n        "example": 0.5\n      },\n      {\n        "name": "presence_penalty",\n        "type": "number",\n        "required": false,\n        "default": 0.0,\n        "description": "Encourages discussing new topics (-2 to 2)",\n        "example": 0.5\n      }\n    ],\n    "outputs": [\n      {\n        "name": "completion",\n        "type": "text",\n        "default": true,\n        "description": "The generated text completion",\n        "example": "\u2022 Key point 1\\n\u2022 Key point 2\\n\u2022 Key point 3"\n      },\n      {\n        "name": "usage",\n        "type": "object",\n        "description": "Token usage statistics for the request",\n        "example": {\n          "prompt_tokens": 100,\n          "completion_tokens": 50,\n          "total_tokens": 150\n        }\n      }\n    ]\n  }\n}\n'})}),"\n",(0,s.jsxs)(n.p,{children:["This task type requires an OpenAI API key to be provided in the workflow execution request headers as ",(0,s.jsx)(n.code,{children:"X-OpenAI-Key"}),"."]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var o=t(6540);const s={},i=o.createContext(s);function r(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);