"use strict";(self.webpackChunkwiki=self.webpackChunkwiki||[]).push([[5013],{7183:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>s,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"task-types/openai-audio-transcription","title":"OpenAI / Audio / Transcription","description":"Workflow task type to call OpenAI\'s Audio Transcription API (speech-to-text model).","source":"@site/docs/task-types/openai-audio-transcription.md","sourceDirName":"task-types","slug":"/task-types/openai-audio-transcription","permalink":"/docs/task-types/openai-audio-transcription","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"OpenAI / Audio / Transcription","slug":"openai-audio-transcription"},"sidebar":"documentationSidebar","previous":{"title":"Task Types","permalink":"/docs/task-types/"},"next":{"title":"Video / Extract Audio","permalink":"/docs/task-types/video-extract-audio"}}');var o=t(4848),r=t(8453);const s={title:"OpenAI / Audio / Transcription",slug:"openai-audio-transcription"},a="OpenAI Whisper",l={},p=[{value:"Inputs",id:"inputs",level:2},{value:"Outputs",id:"outputs",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"openai-whisper",children:"OpenAI Whisper"})}),"\n",(0,o.jsx)(n.p,{children:"Workflow task type to call OpenAI's Audio Transcription API (speech-to-text model)."}),"\n",(0,o.jsx)(n.h2,{id:"inputs",children:"Inputs"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"file"}),": The audio file to transcribe (",(0,o.jsx)(n.strong,{children:"required input"}),")."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"model"}),": The model to use for transcription (",(0,o.jsx)(n.strong,{children:"required input"}),")."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"language"}),": The language of the audio file. Providing language will improve the accuracy of the transcription. Must be in ",(0,o.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes",children:"ISO 639-1"})," format."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"prompt"}),": Optional prompt to guide the model's style or continue a previous audio segment. Should match the language of the audio file."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"response_format"}),": The format of the output, in one of these options: ",(0,o.jsx)(n.code,{children:"json"}),", ",(0,o.jsx)(n.code,{children:"text"}),", ",(0,o.jsx)(n.code,{children:"srt"}),", ",(0,o.jsx)(n.code,{children:"verbose_json"})," or ",(0,o.jsx)(n.code,{children:"vtt"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"temperature"}),": The temperature to use for the model."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"outputs",children:"Outputs"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"text"}),": The transcribed text (",(0,o.jsx)(n.strong,{children:"default output"}),")."]}),"\n"]}),"\n",(0,o.jsx)(n.h1,{id:"example-usage",children:"Example Usage"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "name": "transcribe_audio",\n  "type": "openai.audio.transcription",\n  "inputs": {\n    "file": "{{inputs.file}}",\n    "model": "whisper-1",\n    "language": "en",\n    "prompt": "The audio is a conversation between two people",\n    "response_format": "text",\n    "temperature": 0.2\n  }\n}\n'})}),"\n",(0,o.jsxs)(n.p,{children:["This example assumes that the workflow has an input named ",(0,o.jsx)(n.code,{children:"file"})," that contains an audio file."]}),"\n",(0,o.jsx)(n.h1,{id:"specification",children:"Specification"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "specification": {\n    "type": "openai.audio.transcription",\n    "category": "ai",\n    "description": "Call OpenAI\'s Whisper model to transcribe the audio",\n    "inputs": [\n      {\n        "name": "file",\n        "type": "file",\n        "required": true,\n        "description": "The audio file to transcribe",\n        "example": "<<file>>"\n      },\n      {\n        "name": "model",\n        "type": "text",\n        "required": true,\n        "default": "whisper-1",\n        "description": "The model to use for transcription",\n        "example": "whisper-1"\n      },\n      {\n        "name": "language",\n        "type": "text",\n        "required": false,\n        "description": "The language of the audio file. Providing language will improve the accuracy of the transcription",\n        "example": "en"\n      },\n      {\n        "name": "prompt",\n        "type": "text",\n        "required": false,\n        "description": "Optional prompt to guide the model\'s style or continue a previous audio segment. Should match the language of the audio file",\n        "example": "The audio is a conversation between two people"\n      },\n      {\n        "name": "response_format",\n        "type": "text",\n        "required": false,\n        "description": "The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json` or `vtt`",\n        "example": "text"\n      },\n      {\n        "name": "temperature",\n        "type": "number",\n        "required": false,\n        "default": 0.0,\n        "description": "The temperature to use for the model",\n        "example": "0.0"\n      }\n    ],\n    "outputs": [\n      {\n        "name": "text",\n        "type": "text",\n        "default": true,\n        "description": "The transcribed text",\n        "example": "Hello, how are you?"\n      }\n    ]\n  }\n}\n'})})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var i=t(6540);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);