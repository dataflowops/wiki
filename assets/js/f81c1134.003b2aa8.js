"use strict";(self.webpackChunkwiki=self.webpackChunkwiki||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"workflow-to-download-and-transcribe-youtube-videos-via-api","metadata":{"permalink":"/blog/workflow-to-download-and-transcribe-youtube-videos-via-api","source":"@site/blog/workflow-download-transcribe-youtube-video-api/index.md","title":"Get YouTube Video Transcription with an API call","description":"In this blog post, we will create an automated workflow to download and transcribe YouTube videos via DataFlow Platform\'s API. This tutorial demonstrates building and executing a workflow as standalone automation process accessible via API or web interface.","date":"2025-01-01T21:34:50.000Z","tags":[{"inline":false,"label":"Workflows","permalink":"/blog/tags/workflows","description":"Articles about building, managing, and optimizing workflows"},{"inline":false,"label":"AI Integration","permalink":"/blog/tags/ai-integration","description":"Connecting artificial intelligence with your applications and workflows"},{"inline":false,"label":"Data Flow","permalink":"/blog/tags/data-flow","description":"Articles about the movement and transformation of data within a system or between systems."},{"inline":false,"label":"ETL Processes","permalink":"/blog/tags/etl-processes","description":"Posts related to Extract, Transform, Load processes"},{"inline":false,"label":"App Integration","permalink":"/blog/tags/app-integration","description":"Seamlessly connecting different applications to share data and automate tasks"},{"inline":false,"label":"No-Code AI","permalink":"/blog/tags/no-code-ai","description":"Building AI-powered solutions without writing code"},{"inline":false,"label":"AI Agents","permalink":"/blog/tags/ai-agents","description":"Creating and deploying autonomous AI agents for various tasks"},{"inline":false,"label":"Integrations","permalink":"/blog/tags/integrations","description":"Articles about building, managing, and optimizing integrations"},{"inline":false,"label":"Use Cases","permalink":"/blog/tags/use-cases","description":"Real-world examples of how the Data Flow Platform is used"}],"readingTime":2.77,"hasTruncateMarker":true,"authors":[{"name":"Kanan Rahimov","title":"AppBaza","url":"https://github.com/KenanBek","page":{"permalink":"/blog/authors/kenanbek"},"socials":{"github":"https://github.com/KenanBek","newsletter":"https://codervlogger.com","x":"https://x.com/KenanBekk"},"imageURL":"https://github.com/KenanBek.png","key":"kenanbek"}],"frontMatter":{"slug":"workflow-to-download-and-transcribe-youtube-videos-via-api","title":"Get YouTube Video Transcription with an API call","authors":["kenanbek"],"tags":["workflows","ai-integration","data-flow","etl-processes","app-integration","no-code-ai","ai-agents","integrations","use-cases"]},"unlisted":false,"nextItem":{"title":"Building AI-Powered Workflows with Data Flow Platform","permalink":"/blog/build-ai-workflows-with-data-flow-platform"}},"content":"In this blog post, we will create an automated workflow to download and transcribe YouTube videos via DataFlow Platform\'s API. This tutorial demonstrates building and executing a workflow as standalone automation process accessible via API or web interface.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Workflow\\n\\nThe main execution unit of the DataFlow Platform is a workflow. A workflow is a sequence of tasks that are executed in order.\\n\\nFor this workflow, we need to have the following automation steps:\\n\\n1. Receive a YouTube video URL as an input.\\n2. Download a YouTube video by its URL.\\n3. Extract the video\'s audio.\\n4. Use a cloud AI service to transcribe the audio.\\n5. Return the transcription as a response.\\n\\nUnit of work in DataFlow Platform is a task. A task is a single unit of work that can be executed in a workflow. With the task types, we can implement the automation steps.\\n\\nWe need the following task types to implement this workflow:\\n\\n- **Youtube Downloader**: To download the YouTube video\\n- **Extract Audio from Video**: To extract the video\'s audio\\n- **Call AI Service**: To transcribe the video (OpenAI Whisper)\\n\\nTo have this automation via the platform, we need first to create a workflow unit with all the tasks, and connect these tasks.\\n\\n## Workflow to Automate the Process\\n\\nHere is the initial version of the workflow defined in JSON format:\\n\\n```json\\n{\\n  \\"title\\": \\"YouTube Video Transcription Workflow using Speech-to-Text AI model\\",\\n  \\"description\\": \\"Downloads a YouTube video, extracts audio, transcribes it using OpenAI\'s Whisper model, and returns the transcription text\\",\\n  \\"inputs\\": [\\n    {\\n      \\"name\\": \\"youtube_url\\",\\n      \\"type\\": \\"text\\",\\n      \\"description\\": \\"The URL of the YouTube video to transcribe\\"\\n    }\\n  ],\\n  \\"tasks\\": [\\n    {\\n      \\"name\\": \\"download_video\\",\\n      \\"type\\": \\"youtube.download\\",\\n      \\"inputs\\": {\\n        \\"youtube_url\\": \\"{{inputs.youtube_url}}\\"\\n      }\\n    },\\n    {\\n      \\"name\\": \\"extract_audio\\",\\n      \\"type\\": \\"video.extract_audio\\",\\n      \\"inputs\\": {\\n        \\"video\\": \\"{{tasks.download_video.outputs.video}}\\"\\n      }\\n    },\\n    {\\n      \\"name\\": \\"transcribe_audio\\",\\n      \\"type\\": \\"openai.audio.transcribe\\",\\n      \\"inputs\\": {\\n        \\"file\\": \\"{{tasks.extract_audio.outputs.audio}}\\",\\n        \\"model\\": \\"whisper-1\\"\\n      }\\n    }\\n  ],\\n  \\"outputs\\": [\\n    {\\n      \\"name\\": \\"transcription_text\\",\\n      \\"type\\": \\"text\\",\\n      \\"value\\": \\"{{tasks.transcribe_audio.outputs.text}}\\",\\n      \\"description\\": \\"The transcribed text of the YouTube video\'s audio\\"\\n    }\\n  ]\\n}\\n```\\n\\n## Create a Workflow\\n\\nTo create a workflow, you can use the following command:\\n\\n```bash\\ncurl -X POST \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -d @workflow.json \\\\\\n  https://api.dataflow.wiki/workflows\\n```\\n\\nIf the workflow is created successfully, you will receive a response with the Workflow ID.\\n\\n```json\\n{\\n  \\"workflow_id\\": \\"wf-4g7h8j9k\\"\\n}\\n```\\n\\nSave the Workflow ID, we will need it to execute the workflow.\\n\\n## Execute the Workflow\\n\\nTo execute the workflow, you need to send a POST request to the `/workflows/{{workflow-id}}/executions` endpoint with the input data for the workflow.\\n\\n**API Request**:\\n\\n```bash\\nPOST /workflows/{{workflow-id}}/executions\\n\\nHeaders:\\n- Content-Type: application/json\\n- X-OpenAI-Key: your_openai_api_key\\n```\\n\\n**Request Body**:\\n\\n```json\\n{\\n  \\"inputs\\": {\\n    \\"youtube_url\\": \\"https://www.youtube.com/watch?v=00000000000\\"\\n  }\\n}\\n```\\n\\nThe CURL version of the request would look like this:\\n\\n```bash\\ncurl -X POST \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"X-OpenAI-Key: your_openai_api_key\\" \\\\\\n  -d \'{\\"inputs\\": {\\"youtube_url\\": \\"https://www.youtube.com/watch?v=00000000000\\"}, \\"monitoring\\": {\\"webhook\\": \\"https://example.com/webhook\\"}}\' \\\\\\n  https://api.dataflow.wiki/workflows/{{workflow-id}}/executions\\n```\\n\\nSince this workflow uses OpenAI\'s Whisper model, you must also include your OpenAI API key in the request header.\\n\\n:::info\\n\\nData Flow Platform uses provided API keys only for the tasks that require them and does not store them for future use.\\n\\n:::\\n\\n## Extend\\n\\n- Provide input and output language options\\n- Provide a way to save the transcription to a remote file storage\\n\\nFor detailed documentation and examples, visit our [Documentation Portal](/docs)."},{"id":"build-ai-workflows-with-data-flow-platform","metadata":{"permalink":"/blog/build-ai-workflows-with-data-flow-platform","source":"@site/blog/orchestrate-ai-app-integrations-workflows/index.md","title":"Building AI-Powered Workflows with Data Flow Platform","description":"Learn how to create automated workflows that combine AI models, application integrations, and ETL processes using the Data Flow Platform. This tutorial demonstrates how to build and deploy intelligent workflows as standalone AI agents accessible via API or web interface.","date":"2024-12-18T22:24:24.000Z","tags":[{"inline":false,"label":"Workflows","permalink":"/blog/tags/workflows","description":"Articles about building, managing, and optimizing workflows"},{"inline":false,"label":"AI Integration","permalink":"/blog/tags/ai-integration","description":"Connecting artificial intelligence with your applications and workflows"},{"inline":false,"label":"Data Flow","permalink":"/blog/tags/data-flow","description":"Articles about the movement and transformation of data within a system or between systems."},{"inline":false,"label":"ETL Processes","permalink":"/blog/tags/etl-processes","description":"Posts related to Extract, Transform, Load processes"},{"inline":false,"label":"App Integration","permalink":"/blog/tags/app-integration","description":"Seamlessly connecting different applications to share data and automate tasks"},{"inline":false,"label":"No-Code AI","permalink":"/blog/tags/no-code-ai","description":"Building AI-powered solutions without writing code"},{"inline":false,"label":"AI Agents","permalink":"/blog/tags/ai-agents","description":"Creating and deploying autonomous AI agents for various tasks"},{"inline":false,"label":"Integrations","permalink":"/blog/tags/integrations","description":"Articles about building, managing, and optimizing integrations"}],"readingTime":2.68,"hasTruncateMarker":true,"authors":[{"name":"Kanan Rahimov","title":"AppBaza","url":"https://github.com/KenanBek","page":{"permalink":"/blog/authors/kenanbek"},"socials":{"github":"https://github.com/KenanBek","newsletter":"https://codervlogger.com","x":"https://x.com/KenanBekk"},"imageURL":"https://github.com/KenanBek.png","key":"kenanbek"}],"frontMatter":{"slug":"build-ai-workflows-with-data-flow-platform","title":"Building AI-Powered Workflows with Data Flow Platform","authors":["kenanbek"],"tags":["workflows","ai-integration","data-flow","etl-processes","app-integration","no-code-ai","ai-agents","integrations"]},"unlisted":false,"prevItem":{"title":"Get YouTube Video Transcription with an API call","permalink":"/blog/workflow-to-download-and-transcribe-youtube-videos-via-api"}},"content":"Learn how to create automated workflows that combine AI models, application integrations, and ETL processes using the Data Flow Platform. This tutorial demonstrates how to build and deploy intelligent workflows as standalone AI agents accessible via API or web interface.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Understanding Data Flow Platform Components\\n\\nBefore diving into workflow creation, let\'s understand the key components:\\n\\n1. **Workflow Management**: Create, manage, and monitor workflows.\\n2. **Workflow Execution**: Runtime environment for processing workflow. Based on Directed Acyclic Graphs (DAGs), similar to Apache Airflow.\\n3. **Data Converters**: Handlers for data transformation between different formats.\\n4. **Data Sources**: Connectors for data sources.\\n5. **Data Operations**: Extract, transform, and load data.\\n\\n## Creating Your First Workflow\\n\\n### Design the Workflow Structure\\n\\nThe first step is to design the workflow structure.\\n\\nThe workflow structure is a directed acyclic graph (DAG) that defines the workflow\'s nodes and their connections. We will start with a simple workflow that is sequence of tasks executed one after the other.\\n\\nFor this example, we will create a workflow that Converts Text to Speech (TTS).\\n\\n### Implementing the Text-to-Speech Workflow\\n\\nLet\'s create a workflow that converts text to speech using OpenAI\'s API. Here\'s the workflow definition:\\n\\n```json\\n{\\n  \\"title\\": \\"Text to Speech Workflow\\",\\n  \\"description\\": \\"Converts input text to speech using OpenAI\'s TTS model\\",\\n  \\"inputs\\": [\\n    {\\n      \\"name\\": \\"text\\",\\n      \\"type\\": \\"text\\",\\n      \\"description\\": \\"The text to convert to speech\\"\\n    },\\n    {\\n      \\"name\\": \\"voice\\",\\n      \\"type\\": \\"text\\",\\n      \\"description\\": \\"The voice to use (alloy, echo, fable, onyx, nova, or shimmer)\\",\\n      \\"default\\": \\"alloy\\"\\n    }\\n  ],\\n  \\"tasks\\": [\\n    {\\n      \\"name\\": \\"text_to_speech\\",\\n      \\"type\\": \\"openai.audio.tts\\",\\n      \\"inputs\\": {\\n        \\"text\\": \\"{{inputs.text}}\\",\\n        \\"voice\\": \\"{{inputs.voice}}\\",\\n        \\"model\\": \\"tts-1\\"\\n      }\\n    }\\n  ],\\n  \\"outputs\\": [\\n    {\\n      \\"name\\": \\"audio_file\\",\\n      \\"type\\": \\"file\\",\\n      \\"value\\": \\"{{tasks.text_to_speech.outputs.audio}}\\",\\n      \\"description\\": \\"The generated audio file\\"\\n    }\\n  ]\\n}\\n```\\n\\n### Executing the Workflow\\n\\nOnce you\'ve created the workflow, you can execute it using the API:\\n\\n```bash\\n# First, create the workflow\\ncurl -X POST https://api.dataflow.wiki/workflows \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -d @workflow.json\\n\\n# Execute the workflow with input\\ncurl -X POST https://api.dataflow.wiki/workflows/wf-123/executions \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -d \'{\\n    \\"inputs\\": {\\n      \\"text\\": \\"Hello world! This is a test of the text to speech system.\\",\\n      \\"voice\\": \\"nova\\"\\n    }\\n  }\'\\n```\\n\\n## Monitoring and Error Handling\\n\\n### Setting Up Webhooks\\n\\nTo monitor workflow execution, you can configure webhooks:\\n\\n```json\\n{\\n  \\"monitoring\\": {\\n    \\"webhook\\": {\\n      \\"url\\": \\"https://your-server.com/webhook\\",\\n      \\"events\\": [\\"started\\", \\"completed\\", \\"failed\\"]\\n    }\\n  }\\n}\\n```\\n\\n### Error Handling Strategies\\n\\n1. **Retry Logic**: Configure retry attempts for failed tasks:\\n\\n```json\\n{\\n  \\"name\\": \\"api_call\\",\\n  \\"type\\": \\"http.request\\",\\n  \\"retry\\": {\\n    \\"attempts\\": 3,\\n    \\"delay\\": 5\\n  }\\n}\\n```\\n\\n2. **Fallback Tasks**: Define alternative tasks to execute on failure:\\n\\n```json\\n{\\n  \\"name\\": \\"transcribe\\",\\n  \\"type\\": \\"openai.audio.transcription\\",\\n  \\"fallback\\": {\\n    \\"task\\": \\"whisper.transcribe\\",\\n    \\"inputs\\": {\\n      \\"audio\\": \\"{{tasks.extract_audio.outputs.audio}}\\"\\n    }\\n  }\\n}\\n```\\n\\n## Conclusion\\n\\nThe Data Flow Platform provides a powerful framework for building AI-powered workflows. By combining different task types and using features like parallel execution, conditional logic, and error handling, you can create sophisticated automation solutions that integrate various AI services and data processing steps.\\n\\nStart small with simple workflows and gradually build more complex solutions as you become familiar with the platform\'s capabilities. Remember to implement proper monitoring and error handling to ensure your workflows run reliably in production.\\n\\nFor more examples and detailed documentation, visit our [documentation site](/docs)."}]}}')}}]);